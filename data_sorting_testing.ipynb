{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import iglob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "DATA_AUDIO_DIR = './test'\n",
    "OUTPUT_DIR = './output'\n",
    "TEST_DIR = './Voices_mult/test'\n",
    "TRAIN_DIR = './Voices_mult/train'\n",
    "OUTPUT_TRAIN = os.path.join(OUTPUT_DIR, 'train')\n",
    "OUTPUT_TEST = os.path.join(OUTPUT_DIR, 'test')\n",
    "AUDIO_LENGTH = 10000\n",
    "FINAL_SR = 8000\n",
    "manipulation_const = 0.002\n",
    "\n",
    "class_ids = {\n",
    "    'one': 0,\n",
    "    'two': 1,\n",
    "    'three': 2,\n",
    "    'four': 3,\n",
    "    'five': 4,\n",
    "}\n",
    "def extract_id(wav_filename):\n",
    "    if '1' in wav_filename:\n",
    "        return class_ids.get('one')\n",
    "    elif '2' in wav_filename:\n",
    "        return class_ids.get('two')\n",
    "    elif '3' in wav_filename:\n",
    "        return class_ids.get('three')\n",
    "    elif '4' in wav_filename:\n",
    "        return class_ids.get('four')\n",
    "    elif '5' in wav_filename:\n",
    "        return class_ids.get('five')\n",
    "    else:\n",
    "        return class_ids.get('unlabelled')\n",
    "\n",
    "def read_audio_from_filename(filename, final_sr):\n",
    "    audio, _ = librosa.load(filename, sr=final_sr, mono=True)\n",
    "    audio = audio.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def manipulate(data):\n",
    "        noise = np.random.randn(len(data))\n",
    "        augmented_data = data + manipulation_const * noise\n",
    "        augmented_data = augmented_data.astype(type(data[0]))\n",
    "        return augmented_data\n",
    "\n",
    "for i, wav_filename in enumerate(iglob(os.path.join('DATA_AUDIO_DIR', '**/**.wav'), recursive=True)):                 \n",
    "        print(wav_filename)\n",
    "        class_id = extract_id(wav_filename)\n",
    "        file_name = os.path.splitext(os.path.basename(wav_filename))[0]\n",
    "        dest_path = os.path.dirname(wav_filename)\n",
    "        \n",
    "        output_folder = './pkls'\n",
    "        \n",
    "        audio, sr = librosa.load(wav_filename, mono=True)\n",
    "        \n",
    "        for j in range(len(audios)): \n",
    "            output_filename = os.path.join(output_folder, file_name + str(j) + '.wav')\n",
    "            librosa.output.write_wav(output_filename, audio, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./test\\1 achi49jb.wav 18391 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "1 ./test\\1 lasha62hb.wav 22106 0.0 0.9999999\n",
      "CUT New length = 10000\n",
      "2 ./test\\1 mamuka68hb.wav 18205 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "3 ./test\\1 me_ @^.wav 30465 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "4 ./test\\1 saba£##.wav 19134 0.0 0.9999999\n",
      "CUT New length = 10000\n",
      "5 ./test\\1_ana.wav 5803 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "6 ./test\\1_giorgi.wav 6827 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "7 ./test\\1_nika.wav 7510 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "8 ./test\\1_nina.wav 8875 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "9 ./test\\1_papa.wav 8363 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "10 ./test\\1_valera.wav 9217 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "11 ./test\\1_vato.wav 11265 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "12 ./test\\1_vatu.wav 7339 0.0 1.0\n",
      "PAD New length = 10000\n",
      "13 ./test\\2 achi48jj.wav 13189 0.0 0.99999994\n",
      "CUT New length = 10000\n",
      "14 ./test\\2 lasha72hb.wav 32137 0.0 0.99999994\n",
      "CUT New length = 10000\n",
      "15 ./test\\2 mamuka38hb.wav 18205 0.0 1.0\n",
      "CUT New length = 10000\n",
      "16 ./test\\2 me€_.wav 38081 0.0 1.0\n",
      "CUT New length = 10000\n",
      "17 ./test\\2 saba278.wav 23778 0.0 1.0\n",
      "CUT New length = 10000\n",
      "18 ./test\\2_ana.wav 7168 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "19 ./test\\2_giorgi.wav 7851 -0.0 0.99999994\n",
      "PAD New length = 10000\n",
      "20 ./test\\2_nika.wav 5974 0.0 0.99999994\n",
      "PAD New length = 10000\n",
      "21 ./test\\2_nina.wav 9899 -0.0 0.99999994\n",
      "PAD New length = 10000\n",
      "22 ./test\\2_papa.wav 10411 0.0 1.0000001\n",
      "CUT New length = 10000\n",
      "23 ./test\\2_valera.wav 9387 -0.0 0.99999994\n",
      "PAD New length = 10000\n",
      "24 ./test\\2_vato.wav 12630 0.0 1.0\n",
      "CUT New length = 10000\n",
      "25 ./test\\2_vatu.wav 7510 -0.0 0.99999994\n",
      "PAD New length = 10000\n",
      "26 ./test\\3 achi73bb.wav 14676 0.0 1.0\n",
      "CUT New length = 10000\n",
      "27 ./test\\3 lasha73bb.wav 28607 0.0 1.0\n",
      "CUT New length = 10000\n",
      "28 ./test\\3 mamuka397bne.wav 20806 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "29 ./test\\3 me_@!.wav 39567 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "30 ./test\\3 saba628r.wav 24335 0.0 1.0\n",
      "CUT New length = 10000\n",
      "31 ./test\\3_giorgi.wav 6315 0.0 1.0\n",
      "PAD New length = 10000\n",
      "32 ./test\\3_nika.wav 10240 0.0 0.99999994\n",
      "CUT New length = 10000\n",
      "33 ./test\\3_nina.wav 8363 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "34 ./test\\3_papa.wav 12800 -0.0 0.9999999\n",
      "CUT New length = 10000\n",
      "35 ./test\\3_valera.wav 9899 0.0 1.0\n",
      "PAD New length = 10000\n",
      "36 ./test\\3_vato.wav 14848 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "37 ./test\\3_vatu.wav 8875 -0.0 0.99999994\n",
      "PAD New length = 10000\n",
      "38 ./test\\4 achi39jn.wav 30094 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "39 ./test\\4 lasha78sb.wav 21920 0.0 1.0\n",
      "CUT New length = 10000\n",
      "40 ./test\\4 mamuka68hv.wav 21549 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "41 ./test\\4 me!×÷.wav 29908 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "42 ./test\\4 saba83hh.wav 22477 0.0 1.0\n",
      "CUT New length = 10000\n",
      "43 ./test\\4_ana.wav 6998 -0.0 0.9999998\n",
      "PAD New length = 10000\n",
      "44 ./test\\4_giorgi.wav 8705 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "45 ./test\\4_nika.wav 7339 0.0 1.0\n",
      "PAD New length = 10000\n",
      "46 ./test\\4_papa.wav 10753 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "47 ./test\\4_valera.wav 11094 -0.0 1.0\n",
      "CUT New length = 10000\n",
      "48 ./test\\4_vato.wav 14337 0.0 1.0\n",
      "CUT New length = 10000\n",
      "49 ./test\\4_vatu.wav 8875 0.0 1.0\n",
      "PAD New length = 10000\n",
      "50 ./test\\5 achi49kg.wav 18391 0.0 0.99999994\n",
      "CUT New length = 10000\n",
      "51 ./test\\5 lasha83jn.wav 17648 0.0 1.0\n",
      "CUT New length = 10000\n",
      "52 ./test\\5 mamuka68hv.wav 26564 0.0 1.0\n",
      "CUT New length = 10000\n",
      "53 ./test\\5 me¥^_.wav 19505 0.0 1.0\n",
      "CUT New length = 10000\n",
      "54 ./test\\5 saba739f.wav 22849 -0.0 1.0000001\n",
      "CUT New length = 10000\n",
      "55 ./test\\5_ana.wav 6827 0.0 1.0\n",
      "PAD New length = 10000\n",
      "56 ./test\\5_giorgi.wav 7168 0.0 1.0\n",
      "PAD New length = 10000\n",
      "57 ./test\\5_nika.wav 5462 -0.0 1.0\n",
      "PAD New length = 10000\n",
      "58 ./test\\5_nina.wav 5462 0.0 1.0\n",
      "PAD New length = 10000\n",
      "59 ./test\\5_papa.wav 12118 0.0 1.0\n",
      "CUT New length = 10000\n",
      "60 ./test\\5_valera.wav 9899 -0.0 0.9999999\n",
      "PAD New length = 10000\n",
      "61 ./test\\5_vato.wav 12118 0.0 1.0\n",
      "CUT New length = 10000\n",
      "62 ./test\\5_vatu.wav 6827 0.0 1.0\n",
      "PAD New length = 10000\n"
     ]
    }
   ],
   "source": [
    "DATA_AUDIO_DIR = './test'\n",
    "OUTPUT_DIR = './pkls'\n",
    "import random\n",
    "\n",
    "\n",
    "def extract_id(wav_filename):\n",
    "    if '1' in wav_filename:\n",
    "        return class_ids.get('one')\n",
    "    elif '2' in wav_filename:\n",
    "        return class_ids.get('two')\n",
    "    elif '3' in wav_filename:\n",
    "        return class_ids.get('three')\n",
    "    elif '4' in wav_filename:\n",
    "        return class_ids.get('four')\n",
    "    elif '5' in wav_filename:\n",
    "        return class_ids.get('five')\n",
    "    else:\n",
    "        return class_ids.get('unlabelled')\n",
    "\n",
    "def read_audio_from_filename(filename, final_sr):\n",
    "    audio, _ = librosa.load(filename, sr=final_sr, mono=True)\n",
    "    audio = audio.reshape(-1, 1)\n",
    "    return audio\n",
    "\n",
    "def prepare_all_data():\n",
    "    for i, wav_filename in enumerate(iglob(os.path.join(DATA_AUDIO_DIR, '**/**.wav'), recursive=True)):\n",
    "        class_id = extract_id(wav_filename)\n",
    "        audio_b = read_audio_from_filename(wav_filename, final_sr=FINAL_SR)\n",
    "        audio_buf_final = (audio_b - np.mean(audio_b)) / np.std(audio_b)\n",
    "        original_length = len(audio_buf_final)\n",
    "        print(i, wav_filename, original_length, np.round(np.mean(audio_buf_final), 4), np.std(audio_buf_final))\n",
    "        \n",
    "        if original_length < AUDIO_LENGTH:\n",
    "            audio_buf_final = np.concatenate((audio_buf_final, np.zeros(shape=(AUDIO_LENGTH - original_length, 1))))\n",
    "            print('PAD New length =', len(audio_buf_final))\n",
    "        elif original_length > AUDIO_LENGTH:\n",
    "            audio_buf_final = audio_buf_final[0:AUDIO_LENGTH]\n",
    "            print('CUT New length =', len(audio_buf_final))\n",
    "\n",
    "        output_folder = OUTPUT_DIR\n",
    "\n",
    "        output_filename = os.path.join(output_folder, str(i) + '.pkl')\n",
    "        file_name = os.path.splitext(os.path.basename(wav_filename))[0]\n",
    "        pkl_out = {'class_id': class_id,  'audio': audio_buf_final, 'sr': FINAL_SR}\n",
    "        with open(output_filename, 'wb') as w:\n",
    "            pickle.dump(pkl_out, w)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prepare_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train.shape = (63, 10000, 1)\n",
      "y train.shape = (63, 5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "AUDIO_LENGTH = 10000\n",
    "OUTPUT_DIR = './output'\n",
    "OUTPUT_TRAIN = './pkls'\n",
    "OUTPUT_TEST = os.path.join(OUTPUT_DIR, 'test')\n",
    "n_class = 5\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "\n",
    "def get_my_data_pair(file_list):\n",
    "    def put_into(currfilename, currx, curry):\n",
    "        with open(currfilename, 'rb') as f:\n",
    "            audio_element = pickle.load(f)\n",
    "            currx.append(audio_element['audio'])\n",
    "            curry.append(int(audio_element['class_id']))\n",
    "    x = []\n",
    "    y = []\n",
    "    for filename in file_list:\n",
    "        put_into(filename, x, y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    x_train, y_train = get_my_data_pair(glob(os.path.join(OUTPUT_TRAIN, '**.pkl')))\n",
    "    y_train = to_categorical(y_train, num_classes=5)\n",
    "\n",
    "    print('x train.shape =', x_train.shape)\n",
    "    print('y train.shape =', y_train.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model('best-trained-model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 11ms/step\n",
      "[2.652769550444588, 0.7301587462425232]\n"
     ]
    }
   ],
   "source": [
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = model.evaluate(x_train, y_train, verbose=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>1.148954e-05</td>\n",
       "      <td>2.345406e-09</td>\n",
       "      <td>4.759675e-04</td>\n",
       "      <td>3.496398e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>4.831391e-07</td>\n",
       "      <td>3.716163e-09</td>\n",
       "      <td>5.389706e-06</td>\n",
       "      <td>7.983446e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>8.238702e-07</td>\n",
       "      <td>3.219144e-06</td>\n",
       "      <td>9.000295e-08</td>\n",
       "      <td>2.817931e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.850851e-10</td>\n",
       "      <td>7.085145e-10</td>\n",
       "      <td>4.170606e-07</td>\n",
       "      <td>1.904299e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.961459</td>\n",
       "      <td>1.952499e-06</td>\n",
       "      <td>3.106363e-02</td>\n",
       "      <td>7.475703e-03</td>\n",
       "      <td>1.384383e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>2.945321e-05</td>\n",
       "      <td>2.176905e-05</td>\n",
       "      <td>3.626414e-04</td>\n",
       "      <td>9.994900e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>1.950179e-03</td>\n",
       "      <td>1.736360e-04</td>\n",
       "      <td>7.933011e-03</td>\n",
       "      <td>9.888614e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>4.544995e-05</td>\n",
       "      <td>1.637857e-06</td>\n",
       "      <td>3.521904e-05</td>\n",
       "      <td>3.053203e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.999837</td>\n",
       "      <td>4.343206e-05</td>\n",
       "      <td>4.426820e-05</td>\n",
       "      <td>5.609370e-06</td>\n",
       "      <td>7.000758e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.439522e-08</td>\n",
       "      <td>3.551992e-09</td>\n",
       "      <td>1.083582e-08</td>\n",
       "      <td>3.525354e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0             1             2             3             4\n",
       "0   0.999512  1.148954e-05  2.345406e-09  4.759675e-04  3.496398e-07\n",
       "1   0.999994  4.831391e-07  3.716163e-09  5.389706e-06  7.983446e-10\n",
       "2   0.999996  8.238702e-07  3.219144e-06  9.000295e-08  2.817931e-11\n",
       "3   1.000000  9.850851e-10  7.085145e-10  4.170606e-07  1.904299e-17\n",
       "4   0.961459  1.952499e-06  3.106363e-02  7.475703e-03  1.384383e-09\n",
       "..       ...           ...           ...           ...           ...\n",
       "58  0.000096  2.945321e-05  2.176905e-05  3.626414e-04  9.994900e-01\n",
       "59  0.001082  1.950179e-03  1.736360e-04  7.933011e-03  9.888614e-01\n",
       "60  0.999915  4.544995e-05  1.637857e-06  3.521904e-05  3.053203e-06\n",
       "61  0.999837  4.343206e-05  4.426820e-05  5.609370e-06  7.000758e-05\n",
       "62  1.000000  1.439522e-08  3.551992e-09  1.083582e-08  3.525354e-12\n",
       "\n",
       "[63 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "prediction = model.predict(x_train)\n",
    "prediction = pd.DataFrame(prediction)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.995122e-01</td>\n",
       "      <td>1.148954e-05</td>\n",
       "      <td>2.345406e-09</td>\n",
       "      <td>4.759675e-04</td>\n",
       "      <td>3.496398e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.999942e-01</td>\n",
       "      <td>4.831391e-07</td>\n",
       "      <td>3.716163e-09</td>\n",
       "      <td>5.389706e-06</td>\n",
       "      <td>7.983446e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.999958e-01</td>\n",
       "      <td>8.238702e-07</td>\n",
       "      <td>3.219144e-06</td>\n",
       "      <td>9.000295e-08</td>\n",
       "      <td>2.817931e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>9.850851e-10</td>\n",
       "      <td>7.085145e-10</td>\n",
       "      <td>4.170606e-07</td>\n",
       "      <td>1.904299e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.614587e-01</td>\n",
       "      <td>1.952499e-06</td>\n",
       "      <td>3.106363e-02</td>\n",
       "      <td>7.475703e-03</td>\n",
       "      <td>1.384383e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.029252e-05</td>\n",
       "      <td>9.999326e-01</td>\n",
       "      <td>4.664560e-06</td>\n",
       "      <td>2.317439e-06</td>\n",
       "      <td>6.051403e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.097588e-02</td>\n",
       "      <td>6.474078e-01</td>\n",
       "      <td>1.654187e-01</td>\n",
       "      <td>8.123965e-02</td>\n",
       "      <td>8.495805e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.863654e-03</td>\n",
       "      <td>9.954631e-01</td>\n",
       "      <td>4.924927e-04</td>\n",
       "      <td>8.952594e-05</td>\n",
       "      <td>9.134290e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.093385e-13</td>\n",
       "      <td>9.664534e-01</td>\n",
       "      <td>5.462179e-11</td>\n",
       "      <td>3.354659e-02</td>\n",
       "      <td>1.934434e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.567475e-08</td>\n",
       "      <td>9.892740e-01</td>\n",
       "      <td>6.853613e-08</td>\n",
       "      <td>7.276147e-07</td>\n",
       "      <td>1.072512e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.030352e-06</td>\n",
       "      <td>9.881918e-01</td>\n",
       "      <td>1.176233e-02</td>\n",
       "      <td>4.431043e-05</td>\n",
       "      <td>4.253784e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.093203e-05</td>\n",
       "      <td>9.999102e-01</td>\n",
       "      <td>4.415133e-05</td>\n",
       "      <td>4.822900e-06</td>\n",
       "      <td>5.953972e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>9.999970e-01</td>\n",
       "      <td>2.894694e-06</td>\n",
       "      <td>1.516790e-08</td>\n",
       "      <td>1.838420e-08</td>\n",
       "      <td>8.311009e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.386298e-08</td>\n",
       "      <td>9.764099e-01</td>\n",
       "      <td>2.233359e-02</td>\n",
       "      <td>1.256477e-03</td>\n",
       "      <td>9.725251e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>6.318027e-06</td>\n",
       "      <td>9.405314e-01</td>\n",
       "      <td>5.946175e-02</td>\n",
       "      <td>5.730603e-07</td>\n",
       "      <td>2.775787e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.090377e-07</td>\n",
       "      <td>9.999372e-01</td>\n",
       "      <td>3.008811e-05</td>\n",
       "      <td>3.262969e-05</td>\n",
       "      <td>3.541983e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.746107e-06</td>\n",
       "      <td>9.962215e-01</td>\n",
       "      <td>3.718651e-03</td>\n",
       "      <td>5.501473e-05</td>\n",
       "      <td>2.372136e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.178212e-18</td>\n",
       "      <td>9.998150e-01</td>\n",
       "      <td>1.819334e-04</td>\n",
       "      <td>3.145369e-06</td>\n",
       "      <td>7.277089e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.767784e-09</td>\n",
       "      <td>9.943069e-01</td>\n",
       "      <td>5.301376e-03</td>\n",
       "      <td>3.917721e-04</td>\n",
       "      <td>3.357231e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.831239e-05</td>\n",
       "      <td>1.315571e-06</td>\n",
       "      <td>9.999803e-01</td>\n",
       "      <td>4.713593e-08</td>\n",
       "      <td>2.999753e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.906246e-08</td>\n",
       "      <td>3.262583e-06</td>\n",
       "      <td>9.999965e-01</td>\n",
       "      <td>7.690852e-08</td>\n",
       "      <td>1.090755e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.200507e-09</td>\n",
       "      <td>8.813526e-02</td>\n",
       "      <td>9.118538e-01</td>\n",
       "      <td>1.085680e-05</td>\n",
       "      <td>1.069827e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.257195e-02</td>\n",
       "      <td>4.034791e-01</td>\n",
       "      <td>3.874430e-01</td>\n",
       "      <td>9.026247e-02</td>\n",
       "      <td>9.624361e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.340157e-02</td>\n",
       "      <td>2.344255e-01</td>\n",
       "      <td>5.331395e-01</td>\n",
       "      <td>1.127760e-01</td>\n",
       "      <td>9.625738e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.805654e-12</td>\n",
       "      <td>1.045360e-14</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.787650e-12</td>\n",
       "      <td>1.420880e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.229697e-03</td>\n",
       "      <td>7.930712e-05</td>\n",
       "      <td>9.935473e-01</td>\n",
       "      <td>3.141789e-03</td>\n",
       "      <td>2.070851e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.054230e-09</td>\n",
       "      <td>2.533627e-08</td>\n",
       "      <td>9.999990e-01</td>\n",
       "      <td>9.497371e-07</td>\n",
       "      <td>4.628811e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>4.028994e-06</td>\n",
       "      <td>4.871049e-04</td>\n",
       "      <td>9.995065e-01</td>\n",
       "      <td>2.382617e-06</td>\n",
       "      <td>2.954027e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.213533e-13</td>\n",
       "      <td>1.340765e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.752565e-08</td>\n",
       "      <td>2.371805e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.759965e-07</td>\n",
       "      <td>1.386183e-06</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>7.906226e-07</td>\n",
       "      <td>1.143386e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.323787e-27</td>\n",
       "      <td>1.633520e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.554498e-14</td>\n",
       "      <td>4.358629e-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.299080e-09</td>\n",
       "      <td>1.036093e-06</td>\n",
       "      <td>9.999979e-01</td>\n",
       "      <td>1.051577e-06</td>\n",
       "      <td>6.406276e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>9.990527e-01</td>\n",
       "      <td>7.119746e-07</td>\n",
       "      <td>2.708322e-10</td>\n",
       "      <td>6.132144e-04</td>\n",
       "      <td>3.334008e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.095672e-01</td>\n",
       "      <td>7.055385e-02</td>\n",
       "      <td>1.519596e-05</td>\n",
       "      <td>6.834152e-01</td>\n",
       "      <td>1.364486e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.795036e-02</td>\n",
       "      <td>6.077707e-01</td>\n",
       "      <td>1.985085e-01</td>\n",
       "      <td>7.807031e-02</td>\n",
       "      <td>8.770017e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>9.858881e-01</td>\n",
       "      <td>3.210564e-06</td>\n",
       "      <td>5.553188e-05</td>\n",
       "      <td>6.442926e-03</td>\n",
       "      <td>7.610208e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.668750e-08</td>\n",
       "      <td>8.913983e-04</td>\n",
       "      <td>9.492215e-11</td>\n",
       "      <td>9.991085e-01</td>\n",
       "      <td>3.139687e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.055100e-06</td>\n",
       "      <td>6.197837e-02</td>\n",
       "      <td>6.461556e-07</td>\n",
       "      <td>9.240029e-01</td>\n",
       "      <td>1.401715e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>5.512688e-07</td>\n",
       "      <td>1.547399e-03</td>\n",
       "      <td>9.950151e-01</td>\n",
       "      <td>3.434817e-03</td>\n",
       "      <td>2.124444e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.883431e-06</td>\n",
       "      <td>1.365325e-04</td>\n",
       "      <td>1.060653e-02</td>\n",
       "      <td>9.892531e-01</td>\n",
       "      <td>2.074908e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.371381e-05</td>\n",
       "      <td>8.557617e-02</td>\n",
       "      <td>3.092878e-01</td>\n",
       "      <td>6.051222e-01</td>\n",
       "      <td>5.978672e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>5.810172e-06</td>\n",
       "      <td>1.331649e-01</td>\n",
       "      <td>1.177789e-04</td>\n",
       "      <td>8.667116e-01</td>\n",
       "      <td>2.524276e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>4.197371e-06</td>\n",
       "      <td>6.384527e-01</td>\n",
       "      <td>3.186612e-01</td>\n",
       "      <td>4.288186e-02</td>\n",
       "      <td>1.821935e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.222309e-17</td>\n",
       "      <td>9.957119e-01</td>\n",
       "      <td>2.459155e-03</td>\n",
       "      <td>1.828884e-03</td>\n",
       "      <td>2.537284e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.786459e-03</td>\n",
       "      <td>1.483664e-03</td>\n",
       "      <td>1.039193e-02</td>\n",
       "      <td>9.853309e-01</td>\n",
       "      <td>7.101699e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>5.616249e-01</td>\n",
       "      <td>3.419427e-01</td>\n",
       "      <td>8.855429e-02</td>\n",
       "      <td>3.734754e-03</td>\n",
       "      <td>4.143358e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.862024e-04</td>\n",
       "      <td>2.896783e-08</td>\n",
       "      <td>1.480705e-10</td>\n",
       "      <td>5.146610e-06</td>\n",
       "      <td>9.997086e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.332607e-06</td>\n",
       "      <td>9.446908e-05</td>\n",
       "      <td>1.193047e-07</td>\n",
       "      <td>1.856333e-04</td>\n",
       "      <td>9.997184e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.116442e-02</td>\n",
       "      <td>6.517822e-01</td>\n",
       "      <td>1.548267e-01</td>\n",
       "      <td>8.133287e-02</td>\n",
       "      <td>9.089378e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.043251e-05</td>\n",
       "      <td>1.567562e-01</td>\n",
       "      <td>7.052476e-01</td>\n",
       "      <td>3.196499e-02</td>\n",
       "      <td>1.060108e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.618197e-06</td>\n",
       "      <td>8.939647e-05</td>\n",
       "      <td>1.538532e-10</td>\n",
       "      <td>1.719088e-04</td>\n",
       "      <td>9.997371e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.668023e-01</td>\n",
       "      <td>6.236297e-01</td>\n",
       "      <td>4.130050e-03</td>\n",
       "      <td>1.045065e-01</td>\n",
       "      <td>9.314994e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.206538e-03</td>\n",
       "      <td>5.119791e-03</td>\n",
       "      <td>4.802304e-03</td>\n",
       "      <td>6.902177e-03</td>\n",
       "      <td>9.809692e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>4.778512e-07</td>\n",
       "      <td>1.097774e-04</td>\n",
       "      <td>1.256377e-05</td>\n",
       "      <td>5.819725e-05</td>\n",
       "      <td>9.998190e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.722828e-03</td>\n",
       "      <td>4.387600e-01</td>\n",
       "      <td>2.312772e-02</td>\n",
       "      <td>1.292150e-02</td>\n",
       "      <td>5.234680e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>4.487637e-03</td>\n",
       "      <td>2.240507e-02</td>\n",
       "      <td>3.124402e-03</td>\n",
       "      <td>2.915649e-02</td>\n",
       "      <td>9.408264e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>9.997590e-01</td>\n",
       "      <td>2.102303e-04</td>\n",
       "      <td>2.822769e-05</td>\n",
       "      <td>2.400821e-06</td>\n",
       "      <td>6.733377e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.666898e-03</td>\n",
       "      <td>6.201295e-01</td>\n",
       "      <td>2.703147e-03</td>\n",
       "      <td>6.664274e-02</td>\n",
       "      <td>3.068577e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>9.606226e-05</td>\n",
       "      <td>2.945321e-05</td>\n",
       "      <td>2.176905e-05</td>\n",
       "      <td>3.626414e-04</td>\n",
       "      <td>9.994900e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.081769e-03</td>\n",
       "      <td>1.950179e-03</td>\n",
       "      <td>1.736360e-04</td>\n",
       "      <td>7.933011e-03</td>\n",
       "      <td>9.888614e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>9.999146e-01</td>\n",
       "      <td>4.544995e-05</td>\n",
       "      <td>1.637857e-06</td>\n",
       "      <td>3.521904e-05</td>\n",
       "      <td>3.053203e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>9.998368e-01</td>\n",
       "      <td>4.343206e-05</td>\n",
       "      <td>4.426820e-05</td>\n",
       "      <td>5.609370e-06</td>\n",
       "      <td>7.000758e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.439522e-08</td>\n",
       "      <td>3.551992e-09</td>\n",
       "      <td>1.083582e-08</td>\n",
       "      <td>3.525354e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1             2             3             4\n",
       "0   9.995122e-01  1.148954e-05  2.345406e-09  4.759675e-04  3.496398e-07\n",
       "1   9.999942e-01  4.831391e-07  3.716163e-09  5.389706e-06  7.983446e-10\n",
       "2   9.999958e-01  8.238702e-07  3.219144e-06  9.000295e-08  2.817931e-11\n",
       "3   9.999996e-01  9.850851e-10  7.085145e-10  4.170606e-07  1.904299e-17\n",
       "4   9.614587e-01  1.952499e-06  3.106363e-02  7.475703e-03  1.384383e-09\n",
       "5   6.029252e-05  9.999326e-01  4.664560e-06  2.317439e-06  6.051403e-08\n",
       "6   2.097588e-02  6.474078e-01  1.654187e-01  8.123965e-02  8.495805e-02\n",
       "7   3.863654e-03  9.954631e-01  4.924927e-04  8.952594e-05  9.134290e-05\n",
       "8   3.093385e-13  9.664534e-01  5.462179e-11  3.354659e-02  1.934434e-12\n",
       "9   6.567475e-08  9.892740e-01  6.853613e-08  7.276147e-07  1.072512e-02\n",
       "10  1.030352e-06  9.881918e-01  1.176233e-02  4.431043e-05  4.253784e-07\n",
       "11  4.093203e-05  9.999102e-01  4.415133e-05  4.822900e-06  5.953972e-09\n",
       "12  9.999970e-01  2.894694e-06  1.516790e-08  1.838420e-08  8.311009e-08\n",
       "13  3.386298e-08  9.764099e-01  2.233359e-02  1.256477e-03  9.725251e-09\n",
       "14  6.318027e-06  9.405314e-01  5.946175e-02  5.730603e-07  2.775787e-08\n",
       "15  1.090377e-07  9.999372e-01  3.008811e-05  3.262969e-05  3.541983e-11\n",
       "16  4.746107e-06  9.962215e-01  3.718651e-03  5.501473e-05  2.372136e-08\n",
       "17  1.178212e-18  9.998150e-01  1.819334e-04  3.145369e-06  7.277089e-13\n",
       "18  3.767784e-09  9.943069e-01  5.301376e-03  3.917721e-04  3.357231e-10\n",
       "19  1.831239e-05  1.315571e-06  9.999803e-01  4.713593e-08  2.999753e-10\n",
       "20  2.906246e-08  3.262583e-06  9.999965e-01  7.690852e-08  1.090755e-12\n",
       "21  1.200507e-09  8.813526e-02  9.118538e-01  1.085680e-05  1.069827e-07\n",
       "22  2.257195e-02  4.034791e-01  3.874430e-01  9.026247e-02  9.624361e-02\n",
       "23  2.340157e-02  2.344255e-01  5.331395e-01  1.127760e-01  9.625738e-02\n",
       "24  2.805654e-12  1.045360e-14  1.000000e+00  4.787650e-12  1.420880e-15\n",
       "25  3.229697e-03  7.930712e-05  9.935473e-01  3.141789e-03  2.070851e-06\n",
       "26  1.054230e-09  2.533627e-08  9.999990e-01  9.497371e-07  4.628811e-11\n",
       "27  4.028994e-06  4.871049e-04  9.995065e-01  2.382617e-06  2.954027e-09\n",
       "28  1.213533e-13  1.340765e-09  1.000000e+00  1.752565e-08  2.371805e-15\n",
       "29  1.759965e-07  1.386183e-06  9.999976e-01  7.906226e-07  1.143386e-12\n",
       "30  2.323787e-27  1.633520e-16  1.000000e+00  3.554498e-14  4.358629e-28\n",
       "31  2.299080e-09  1.036093e-06  9.999979e-01  1.051577e-06  6.406276e-12\n",
       "32  9.990527e-01  7.119746e-07  2.708322e-10  6.132144e-04  3.334008e-04\n",
       "33  1.095672e-01  7.055385e-02  1.519596e-05  6.834152e-01  1.364486e-01\n",
       "34  2.795036e-02  6.077707e-01  1.985085e-01  7.807031e-02  8.770017e-02\n",
       "35  9.858881e-01  3.210564e-06  5.553188e-05  6.442926e-03  7.610208e-03\n",
       "36  3.668750e-08  8.913983e-04  9.492215e-11  9.991085e-01  3.139687e-09\n",
       "37  1.055100e-06  6.197837e-02  6.461556e-07  9.240029e-01  1.401715e-02\n",
       "38  5.512688e-07  1.547399e-03  9.950151e-01  3.434817e-03  2.124444e-06\n",
       "39  3.883431e-06  1.365325e-04  1.060653e-02  9.892531e-01  2.074908e-09\n",
       "40  1.371381e-05  8.557617e-02  3.092878e-01  6.051222e-01  5.978672e-08\n",
       "41  5.810172e-06  1.331649e-01  1.177789e-04  8.667116e-01  2.524276e-09\n",
       "42  4.197371e-06  6.384527e-01  3.186612e-01  4.288186e-02  1.821935e-08\n",
       "43  2.222309e-17  9.957119e-01  2.459155e-03  1.828884e-03  2.537284e-13\n",
       "44  2.786459e-03  1.483664e-03  1.039193e-02  9.853309e-01  7.101699e-06\n",
       "45  5.616249e-01  3.419427e-01  8.855429e-02  3.734754e-03  4.143358e-03\n",
       "46  2.862024e-04  2.896783e-08  1.480705e-10  5.146610e-06  9.997086e-01\n",
       "47  1.332607e-06  9.446908e-05  1.193047e-07  1.856333e-04  9.997184e-01\n",
       "48  2.116442e-02  6.517822e-01  1.548267e-01  8.133287e-02  9.089378e-02\n",
       "49  2.043251e-05  1.567562e-01  7.052476e-01  3.196499e-02  1.060108e-01\n",
       "50  1.618197e-06  8.939647e-05  1.538532e-10  1.719088e-04  9.997371e-01\n",
       "51  2.668023e-01  6.236297e-01  4.130050e-03  1.045065e-01  9.314994e-04\n",
       "52  2.206538e-03  5.119791e-03  4.802304e-03  6.902177e-03  9.809692e-01\n",
       "53  4.778512e-07  1.097774e-04  1.256377e-05  5.819725e-05  9.998190e-01\n",
       "54  1.722828e-03  4.387600e-01  2.312772e-02  1.292150e-02  5.234680e-01\n",
       "55  4.487637e-03  2.240507e-02  3.124402e-03  2.915649e-02  9.408264e-01\n",
       "56  9.997590e-01  2.102303e-04  2.822769e-05  2.400821e-06  6.733377e-08\n",
       "57  3.666898e-03  6.201295e-01  2.703147e-03  6.664274e-02  3.068577e-01\n",
       "58  9.606226e-05  2.945321e-05  2.176905e-05  3.626414e-04  9.994900e-01\n",
       "59  1.081769e-03  1.950179e-03  1.736360e-04  7.933011e-03  9.888614e-01\n",
       "60  9.999146e-01  4.544995e-05  1.637857e-06  3.521904e-05  3.053203e-06\n",
       "61  9.998368e-01  4.343206e-05  4.426820e-05  5.609370e-06  7.000758e-05\n",
       "62  1.000000e+00  1.439522e-08  3.551992e-09  1.083582e-08  3.525354e-12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', prediction.shape[0]+1)\n",
    "\n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy :  0.8219780325889587\n"
     ]
    }
   ],
   "source": [
    "print(\"validation accuracy : \", result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8034 samples, validate on 910 samples\n",
      "Epoch 1/31\n",
      "8034/8034 [==============================] - 401s 50ms/step - loss: 0.2079 - accuracy: 0.9442 - val_loss: 2.1272 - val_accuracy: 0.6099\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.82088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/31\n",
      "8034/8034 [==============================] - 396s 49ms/step - loss: 0.2324 - accuracy: 0.9344 - val_loss: 1.4961 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.82088\n",
      "Epoch 3/31\n",
      "8034/8034 [==============================] - 396s 49ms/step - loss: 0.2326 - accuracy: 0.9361 - val_loss: 2.2179 - val_accuracy: 0.6132\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.82088\n",
      "Epoch 4/31\n",
      "8034/8034 [==============================] - 397s 49ms/step - loss: 0.2059 - accuracy: 0.9470 - val_loss: 1.3132 - val_accuracy: 0.7758\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.82088\n",
      "Epoch 5/31\n",
      "8034/8034 [==============================] - 395s 49ms/step - loss: 0.1878 - accuracy: 0.9515 - val_loss: 0.8038 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.82088\n",
      "Epoch 6/31\n",
      "8034/8034 [==============================] - 396s 49ms/step - loss: 0.1745 - accuracy: 0.9553 - val_loss: 1.2084 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.82088\n",
      "Epoch 7/31\n",
      "8034/8034 [==============================] - 414s 52ms/step - loss: 0.1677 - accuracy: 0.9569 - val_loss: 1.1171 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.82088\n",
      "Epoch 8/31\n",
      "8034/8034 [==============================] - 428s 53ms/step - loss: 0.1648 - accuracy: 0.9594 - val_loss: 1.1187 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.82088\n",
      "Epoch 9/31\n",
      "8034/8034 [==============================] - 397s 49ms/step - loss: 0.2044 - accuracy: 0.9441 - val_loss: 3.1549 - val_accuracy: 0.4989\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.82088\n",
      "Epoch 10/31\n",
      "8034/8034 [==============================] - 435s 54ms/step - loss: 0.2014 - accuracy: 0.9456 - val_loss: 1.7369 - val_accuracy: 0.6615\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.82088\n",
      "Epoch 11/31\n",
      "8034/8034 [==============================] - 514s 64ms/step - loss: 0.1859 - accuracy: 0.9536 - val_loss: 1.0433 - val_accuracy: 0.7758\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.82088\n",
      "Epoch 12/31\n",
      "8034/8034 [==============================] - 521s 65ms/step - loss: 0.2037 - accuracy: 0.9452 - val_loss: 1.8726 - val_accuracy: 0.6725\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.82088\n",
      "Epoch 13/31\n",
      "8034/8034 [==============================] - 523s 65ms/step - loss: 0.1944 - accuracy: 0.9476 - val_loss: 1.3246 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.82088\n",
      "Epoch 14/31\n",
      "8034/8034 [==============================] - 493s 61ms/step - loss: 0.1775 - accuracy: 0.9556 - val_loss: 1.7646 - val_accuracy: 0.7077\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.82088\n",
      "Epoch 15/31\n",
      "8034/8034 [==============================] - 524s 65ms/step - loss: 0.1731 - accuracy: 0.9577 - val_loss: 0.9105 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.82088 to 0.82198, saving model to best-trained-model-new3.hdf5\n",
      "Epoch 16/31\n",
      "8034/8034 [==============================] - 513s 64ms/step - loss: 0.1616 - accuracy: 0.9594 - val_loss: 1.1044 - val_accuracy: 0.7725\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.82198\n",
      "Epoch 17/31\n",
      "8034/8034 [==============================] - 512s 64ms/step - loss: 0.1699 - accuracy: 0.9569 - val_loss: 1.9316 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.82198\n",
      "Epoch 18/31\n",
      "8034/8034 [==============================] - 528s 66ms/step - loss: 0.1576 - accuracy: 0.9620 - val_loss: 1.5297 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.82198\n",
      "Epoch 19/31\n",
      "8034/8034 [==============================] - 503s 63ms/step - loss: 0.1498 - accuracy: 0.9651 - val_loss: 0.9285 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.82198\n",
      "Epoch 20/31\n",
      "8034/8034 [==============================] - 507s 63ms/step - loss: 0.1862 - accuracy: 0.9511 - val_loss: 2.1785 - val_accuracy: 0.6473\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.82198\n",
      "Epoch 21/31\n",
      "8034/8034 [==============================] - 511s 64ms/step - loss: 0.2200 - accuracy: 0.9396 - val_loss: 3.4066 - val_accuracy: 0.5571\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.82198\n",
      "Epoch 22/31\n",
      "8034/8034 [==============================] - 513s 64ms/step - loss: 0.2015 - accuracy: 0.9469 - val_loss: 1.1538 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.82198\n",
      "Epoch 23/31\n",
      "8034/8034 [==============================] - 516s 64ms/step - loss: 0.1682 - accuracy: 0.9600 - val_loss: 0.9275 - val_accuracy: 0.7824\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.82198\n",
      "Epoch 24/31\n",
      "8034/8034 [==============================] - 518s 65ms/step - loss: 0.1575 - accuracy: 0.9629 - val_loss: 1.4535 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.82198\n",
      "Epoch 25/31\n",
      "8034/8034 [==============================] - 520s 65ms/step - loss: 0.1866 - accuracy: 0.9529 - val_loss: 1.4187 - val_accuracy: 0.7462\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.82198\n",
      "Epoch 26/31\n",
      "8034/8034 [==============================] - 504s 63ms/step - loss: 0.1674 - accuracy: 0.9572 - val_loss: 1.1701 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.82198\n",
      "Epoch 27/31\n",
      "8034/8034 [==============================] - 507s 63ms/step - loss: 0.1607 - accuracy: 0.9613 - val_loss: 1.3304 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.82198\n",
      "Epoch 28/31\n",
      "8034/8034 [==============================] - 508s 63ms/step - loss: 0.1575 - accuracy: 0.9627 - val_loss: 4.4258 - val_accuracy: 0.4912\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.82198\n",
      "Epoch 29/31\n",
      "8034/8034 [==============================] - 525s 65ms/step - loss: 0.1759 - accuracy: 0.9574 - val_loss: 3.0531 - val_accuracy: 0.5769\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.82198\n",
      "Epoch 30/31\n",
      "8034/8034 [==============================] - 504s 63ms/step - loss: 0.1819 - accuracy: 0.9547 - val_loss: 1.2636 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.82198\n",
      "Epoch 31/31\n",
      "8034/8034 [==============================] - 510s 64ms/step - loss: 0.1815 - accuracy: 0.9538 - val_loss: 2.9926 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.82198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16fb7a9f708>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, y=y_train, epochs=31, batch_size=batch_size, validation_data=(x_test, y_test), callbacks = callbacks_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
